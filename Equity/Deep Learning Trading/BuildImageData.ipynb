{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_files(dir_path, W, forwardperiod, companies_bundle, isTrain):\n",
    "    str_total = ''\n",
    "    str_total_Y = ''\n",
    "    \n",
    "    for ticker in companies_bundle:\n",
    "        time1 = time.time()\n",
    "        ticker0 = ticker+'.csv'\n",
    "        df = pd.read_csv(os.path.join(dir_path, ticker0))\n",
    "        df.index = df.Date\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        #Resampling\n",
    "        df = df.resample('W',loffset=pd.offsets.timedelta(days=-6)).agg({'Date':'first','Open': 'first', 'High': 'max', 'Low': 'min','Adj Close': 'last', 'Volume': 'sum'})\n",
    "\n",
    "        print('')\n",
    "        print('Length Dataframe {} is {}'.format(ticker0.replace('.csv',''), len(df)))\n",
    "        df['NextRets'] = 100*df['Adj Close'].pct_change().shift(-1)\n",
    "\n",
    "        #Step1: Preprocess data\n",
    "        df = df.dropna()\n",
    "        df = quantilize(df)\n",
    "        \n",
    "        if isTrain == True:\n",
    "            #Step2: Neutralize returns to avoid data imbalance\n",
    "            df['NextRets'] = df['NextRets'] - np.mean(df['NextRets'])\n",
    "   \n",
    "        seq_volumes = list()\n",
    "        seq_closes = list()\n",
    "        returns = list()\n",
    "        volumes = list()\n",
    "        closes = list()\n",
    "        highest_volumes = list()\n",
    "        lowest_volumes = list()\n",
    "        highest_closes = list()\n",
    "        lowest_closes = list()\n",
    "        dates = list()\n",
    "        for i in range(W-1,len(df)-forwardperiod, forwardperiod):\n",
    "            seq_closes.append(np.array(df['Adj Close'].iloc[i-W+1:i+1]))\n",
    "            seq_volumes.append(np.array(df['Volume'].iloc[i-W+1:i+1]))\n",
    "            returns.append(np.array(np.sum([df['NextRets'].iloc[f] for f in range(i,i+forwardperiod)]))) \n",
    "            highest_closes.append(np.max(seq_closes[-1]))\n",
    "            lowest_closes.append(np.min(seq_closes[-1]))\n",
    "            highest_volumes.append(np.max(seq_volumes[-1]))\n",
    "            lowest_volumes.append(np.min(seq_volumes[-1]))\n",
    "            dates.append(df['Date'].iloc[i])\n",
    "\n",
    "        H1 = W//2-1\n",
    "\n",
    "        str_ticker = ''\n",
    "        str_ticker_Y = ''\n",
    "        for i in range(len(seq_closes)):\n",
    "            #Close image generation 15*32\n",
    "            image_close = np.zeros((H1,W)).astype(np.int64)\n",
    "            highest_close = highest_closes[i]\n",
    "            lowest_close = lowest_closes[i]\n",
    "            spread = (highest_close - lowest_close)/H1\n",
    "            seq_closes_i = seq_closes[i]\n",
    "            for j in range(H1):\n",
    "                for k in range(len(seq_closes_i)):\n",
    "                    close_k = seq_closes_i[k]\n",
    "                    if lowest_close+j*spread<=close_k<lowest_close+(j+1)*spread:\n",
    "                        image_close[H1-j-1,k] = 1\n",
    "                    if close_k==lowest_close+H1*spread:\n",
    "                        image_close[0,k] = 1\n",
    "\n",
    "            #Volume image generation 15*32\n",
    "            image_volume = np.zeros((H1,W)).astype(np.int64)\n",
    "            highest_volume = highest_volumes[i]\n",
    "            lowest_volume = lowest_volumes[i]\n",
    "            spread = (highest_volume - lowest_volume)/H1\n",
    "            seq_volumes_i = seq_volumes[i]\n",
    "            for j in range(H1):\n",
    "                for k in range(len(seq_volumes_i)):\n",
    "                    volume_k = seq_volumes_i[k]\n",
    "                    if lowest_volume+j*spread<=volume_k<lowest_volume+(j+1)*spread:\n",
    "                        image_volume[H1-j-1,k] = 1\n",
    "                    if volume_k==lowest_volume+H1*spread:\n",
    "                        image_volume[0,k] = 1\n",
    "\n",
    "            #Whole image\n",
    "            int_image = np.zeros((2,W)).astype(np.int64)\n",
    "            image = np.concatenate((image_close, int_image, image_volume), axis=0)\n",
    "            image = image.astype(str)\n",
    "\n",
    "            #Join\n",
    "            for term in image:\n",
    "                str_ticker+=' '.join(term) + ' \\n'\n",
    "            if i == len(seq_closes) - 1:\n",
    "                str_ticker+= 'F\\n'\n",
    "            else:\n",
    "                str_ticker+= 'E\\n'\n",
    "            str_ticker_Y+= str(np.round(float(returns[i]),4)) + '\\n'\n",
    "\n",
    "        print('Ticker {} completed in {} s'.format(ticker.replace('.csv',''), time.time()-time1))\n",
    "        str_total+=str_ticker\n",
    "        str_total_Y+=str_ticker_Y\n",
    "        str_ticker = ''\n",
    "        str_ticker_Y = ''\n",
    "    \n",
    "    string = ''\n",
    "    for company in companies_bundle:\n",
    "        string+='_'+str(company)\n",
    "        \n",
    "    with open(os.path.join(dir_path,'inputX'+string+'.txt'), 'w') as inputXfile:\n",
    "        inputXfile.write(str_total)\n",
    "    inputXfile.close()\n",
    "\n",
    "    with open(os.path.join(dir_path,'inputY'+string+'.txt'), 'w') as inputYfile:\n",
    "        inputYfile.write(str_total_Y)                      \n",
    "    inputYfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "W=16\n",
    "forwardperiod = 4\n",
    "country_ticker = 'US'\n",
    "base_path = r'YourSP500DataPath'\n",
    "path = os.path.join(base_path,country_ticker)\n",
    "concatdates = '1999_2016' #Could be either 1999_2016 or 2000_2016. Need to check\n",
    "dir_path = os.path.join(path,'Train','W_'+concatdates)\n",
    "\n",
    "for companies_bundle in true_companies_bundles:\n",
    "    dir_path = os.path.join(path,'Train','W_'+concatdates)\n",
    "\n",
    "    #Create images\n",
    "    create_image_files(dir_path, W, forwardperiod, companies_bundle, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "W=16\n",
    "forwardperiod = 4\n",
    "country_ticker = 'US'\n",
    "base_path = r'YourSP500DataPath'\n",
    "path = os.path.join(base_path,country_ticker)\n",
    "concatdates = '2016_2020'\n",
    "dir_path = os.path.join(path,'Test','W_'+concatdates)\n",
    "\n",
    "for companies_bundle in true_companies_bundles:\n",
    "    dir_path = os.path.join(path,'Test','W_'+concatdates)\n",
    "\n",
    "    #Create images\n",
    "    create_image_files(dir_path, W, forwardperiod, companies_bundle, isTrain=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
