{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference\n",
    "# Global Stock Market Prediction Based on Stock Chart Images Using Deep Q-Network,\n",
    "# Authors: Jinho Lee, Raehyun Kim, Yookyung Koh, and Jaewoo Kang,\n",
    "# URL https://arxiv.org/abs/1902.10948\n",
    "\n",
    "#Obtain SP500 tickers\n",
    "import urllib.request\n",
    "from html_table_parser import HTMLTableParser\n",
    "\n",
    "url_snp500 = 'http://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "def obtain_parse_wiki_stocks_sp500(url):\n",
    "    \"\"\"Download and parse the Wikipedia list of S&P500 \n",
    "    constituents using requests and libxml.\n",
    "\n",
    "    Returns a list of tuples for to add to MySQL.\"\"\"\n",
    "\n",
    "    # Get S&P500 website content\n",
    "    req = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    data = response.read().decode('utf-8')\n",
    "\n",
    "    #Instantiate the parser and feed it\n",
    "    p = HTMLTableParser()\n",
    "    p.feed(data)\n",
    "    table_list = p.tables\n",
    "    table = table_list[0][1:]\n",
    "\n",
    "    # Obtain the symbol information for each row in the S&P500 constituent table\n",
    "    symbols = []\n",
    "    for row in table:\n",
    "        sd = {'ticker': row[0],\n",
    "            'name': row[1],\n",
    "            'sector': row[3]}\n",
    "        # Create a tuple (for the DB format) and append to the grand list\n",
    "        symbols.append(sd['ticker'])\n",
    "    return symbols\n",
    "\n",
    "snp500_tickers = obtain_parse_wiki_stocks_sp500(url_snp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_bundles = list()\n",
    "for i in range(0,len(snp500_tickers),5):\n",
    "    companies_bundles.append(snp500_tickers[i:i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'YourSP500DataPath'\n",
    "\n",
    "def build_ticker_dataset(path, ticker, start, end):\n",
    "    \"\"\"\n",
    "    Creates the dataset containing all stock prices\n",
    "    :returns: stock_prices.csv\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all Adjusted Close prices for all the tickers in our list,\n",
    "    # between START_DATE and END_DATE\n",
    "    data = pdr.get_data_yahoo(ticker, start, end)\n",
    "    if len(data.dropna())>0.75*len(data):\n",
    "        data.to_csv(os.path.join(path,ticker+'.csv'))\n",
    "\n",
    "def quantilize(df):\n",
    "    #Find the 1st and the 3rd of each column Q1 and Q3\n",
    "    df_quantiles = df.quantile([0.25,0.75])\n",
    "    \n",
    "    #Adjust(symmetrize) columns distribution using the interval\n",
    "    #[Q1-1.5*IQR,Q3+1.5*IQR] where IQR = Q3 - Q1\n",
    "    \n",
    "    df_adj = pd.DataFrame(columns = df.columns, index = df.index)\n",
    "    df_adj['Date'] = df['Date']\n",
    "    \n",
    "    cols = df_quantiles.columns\n",
    "    for col in cols:\n",
    "        Q1 = df_quantiles.loc[0.25][col]\n",
    "        Q3 = df_quantiles.loc[0.75][col]\n",
    "        IQR = Q3 - Q1\n",
    "        B1 = Q1 - 2.5*IQR ; B2 = Q3 + 2.5*IQR\n",
    "        \n",
    "        df_adj[col] = np.where((df[col] > B1) & (df[col] < B2), df[col], np.where((df[col] < B1), B1, B2))\n",
    "    df_adj['Volume'] = df_adj['Volume'].astype(np.int64)\n",
    "    \n",
    "    return df_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ticker = 'US'\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2020-12-31\"\n",
    "\n",
    "true_companies_bundles = list()\n",
    "for companies_bundle in companies_bundles:\n",
    "    for ticker in companies_bundle:\n",
    "        print(ticker)\n",
    "        path = os.path.join(base_path,country_ticker)\n",
    "        concatdates = start_date[:4]+'_'+str(int(end_date[:4])+1)\n",
    "\n",
    "        dir_path1 = os.path.join(path,'W_'+concatdates)\n",
    "        if not os.path.exists(dir_path1):\n",
    "            os.makedirs(dir_path1)   \n",
    "        build_ticker_dataset(dir_path1, ticker, start_date, end_date)\n",
    "    \n",
    "    dictlens = dict()\n",
    "    for ticker in companies_bundle:\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(dir_path1, ticker+'.csv'))\n",
    "            l = len(df.dropna())\n",
    "            dictlens[l] = dictlens.get(l,0)+1\n",
    "        except FileNotFoundError:\n",
    "            companies_bundle.remove(ticker)\n",
    "\n",
    "    mostrecurrentlen = 0\n",
    "    numbrecs = 0\n",
    "    for l in dictlens.keys():\n",
    "        if dictlens[l] >numbrecs:\n",
    "            numbrecs = dictlens[l]\n",
    "            mostrecurrentlen = l\n",
    "    \n",
    "    tickers_to_consider = list()\n",
    "    for ticker in companies_bundle:\n",
    "        df = pd.read_csv(os.path.join(dir_path1, ticker+'.csv'))\n",
    "        if len(df.dropna())==mostrecurrentlen:\n",
    "            tickers_to_consider.append(ticker)\n",
    "    if len(tickers_to_consider)>1:\n",
    "        true_companies_bundles.append(tickers_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for companies_bundle in true_companies_bundles:\n",
    "    print('')\n",
    "    print('Companies bundle {} started'.format(companies_bundle))\n",
    "    for ticker in companies_bundle:\n",
    "\n",
    "        #Split train_test\n",
    "        ticker0 = ticker+'.csv'\n",
    "        df = pd.read_csv(os.path.join(dir_path1, ticker0))\n",
    "        df = df.dropna()\n",
    "        n = int(0.0*len(df))\n",
    "        l = int(0.8*len(df))\n",
    "        f = int(0.2*len(df))\n",
    "        df_train, df_test = df[n:n+l], df[n+l:n+l+f]\n",
    "\n",
    "        #Train\n",
    "        start_train_date = list(df_train['Date'])[0]\n",
    "        end_train_date = list(df_train['Date'])[-1]\n",
    "\n",
    "        concatdates = start_train_date[:4]+'_'+str(int(end_train_date[:4]))\n",
    "        dir_path2 = os.path.join(path,'Train','W_'+concatdates)\n",
    "\n",
    "        if not os.path.exists(dir_path2):\n",
    "            os.makedirs(dir_path2)   \n",
    "\n",
    "        df_train.to_csv(os.path.join(dir_path2,ticker0))\n",
    "\n",
    "        #Test\n",
    "        start_test_date = list(df_test['Date'])[0]\n",
    "        end_test_date = list(df_test['Date'])[-1]\n",
    "\n",
    "        concatdates = start_test_date[:4]+'_'+str(int(end_test_date[:4]))\n",
    "        dir_path3 = os.path.join(path,'Test','W_'+concatdates)\n",
    "\n",
    "        if not os.path.exists(dir_path3):\n",
    "            os.makedirs(dir_path3)   \n",
    "\n",
    "        df_test.to_csv(os.path.join(dir_path3,ticker0))\n",
    "                \n",
    "    print('')\n",
    "    print('Companies bundle {} completed'.format(companies_bundle))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
